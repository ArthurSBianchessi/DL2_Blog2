{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn             as nn\n",
    "import torch.nn.functional  as F\n",
    "from torch.utils.data       import DataLoader, random_split, Dataset\n",
    "from torchvision            import transforms\n",
    "from torchvision.datasets   import MNIST\n",
    "\n",
    "import lightning                    as     L\n",
    "from   lightning.pytorch.callbacks  import EarlyStopping\n",
    "from   lightning.pytorch.callbacks  import TQDMProgressBar, ModelCheckpoint\n",
    "from   lightning.pytorch.loggers    import TensorBoardLogger, CSVLogger\n",
    "\n",
    "import numpy            as np\n",
    "import pandas           as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 1),             # 28x28 -> 32x26x26\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                    # 32x26x26 -> 32x13x13\n",
    "            nn.Conv2d(32, 64, 3, 1),            # 32x13x13 -> 64x11x11\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                    # 64x11x11 -> 64x5x5\n",
    "            nn.Conv2d(64, 128, 3, 1),           # 64x5x5 -> 128x3x3\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 3, 1),          # 128x3x3 -> 256x1x1\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),                       # 256x1x1 -> 256\n",
    "            nn.Linear(256, 128),                # 256 -> 128\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),                 # 128 -> 10\n",
    "            # nn.Flatten(),                       # 64x5x5 -> 1600\n",
    "            # nn.Linear(1600, 128),               # 1600 -> 128\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(128, 10),                 # 128 -> 10\n",
    "        )\n",
    "\n",
    "        self.criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "    def _common_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        return y_hat, self.criteria(y_hat, y)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y_hat, loss = self._common_step(batch, batch_idx)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y_hat, loss = self._common_step(batch, batch_idx)\n",
    "        self.log('val_loss', loss)\n",
    "        # Log accuracy\n",
    "        pred = torch.argmax(y_hat, dim=1)\n",
    "        acc = torch.sum(pred == batch[1]).item() / len(pred)\n",
    "        self.log('val_acc', acc)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        y_hat, loss = self._common_step(batch, batch_idx)\n",
    "        self.log('test_loss', loss)\n",
    "        # Log accuracy\n",
    "        pred = torch.argmax(y_hat, dim=1)\n",
    "        acc = torch.sum(pred == batch[1]).item() / len(pred)\n",
    "        self.log('test_acc', acc)\n",
    "        return loss\n",
    "    \n",
    "    # def val dataloader(self):\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(MNIST(root='.', train=False, download=True, transform=transforms.ToTensor()), batch_size=32)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(MNIST(root='.', train=True, download=True, transform=transforms.ToTensor()), batch_size=32)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 860 batches, Val: 79 batches, Test: 157 batches\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset   = random_split(MNIST(root='.', train=True, transform=transforms.ToTensor(), download=True), [55000, 5000])\n",
    "test_dataset                = MNIST(root='.', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "train_loader                = DataLoader(train_dataset, batch_size=64, shuffle=True,  num_workers=4, persistent_workers=True)\n",
    "val_loader                  = DataLoader(val_dataset,   batch_size=64, shuffle=False, num_workers=4, persistent_workers=True)\n",
    "test_loader                 = DataLoader(test_dataset,  batch_size=64, shuffle=False, num_workers=4, persistent_workers=True)\n",
    "print(f\"Train: {len(train_loader)} batches, Val: {len(val_loader)} batches, Test: {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/abianche/miniconda3/envs/lightning/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "Missing logger folder: /Users/abianche/Library/CloudStorage/OneDrive-AutomaticDataProcessingInc/Documentos/ARTHUR/DL2_Blog2/lightning_logs\n",
      "\n",
      "  | Name     | Type             | Params\n",
      "----------------------------------------------\n",
      "0 | model    | Sequential       | 422 K \n",
      "1 | criteria | CrossEntropyLoss | 0     \n",
      "----------------------------------------------\n",
      "422 K     Trainable params\n",
      "0         Non-trainable params\n",
      "422 K     Total params\n",
      "1.688     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa2b6bb596447dab8f784051601789c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b621a5941b0b4a049c50c519ad191636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model               = ConvNet()\n",
    "\n",
    "trainer             = L.Trainer()\n",
    "\n",
    "trainer.fit(model)\n",
    "# trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name     | Type             | Params\n",
      "----------------------------------------------\n",
      "0 | model    | Sequential       | 422 K \n",
      "1 | criteria | CrossEntropyLoss | 0     \n",
      "----------------------------------------------\n",
      "422 K     Trainable params\n",
      "0         Non-trainable params\n",
      "422 K     Total params\n",
      "1.688     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ecb0ee304b4604aa0226fbdf42031d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da910213d2aa47c583e232c333e1ca91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210974d76bb1462f87a3959d0f5a55aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4882bfa690e5491699f3bf91c44d5340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model               = ConvNet()\n",
    "\n",
    "trainer             = L.Trainer(\n",
    "                            max_epochs=100, accelerator='mps', enable_progress_bar=True, devices=1,\n",
    "                            callbacks=[\n",
    "                                EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=1), \n",
    "                                TQDMProgressBar(refresh_rate=10),\n",
    "                                ModelCheckpoint(\n",
    "                                                # dirpath='logs/checkpoints/',\n",
    "\n",
    "                                                filename='{epoch}_{step}_{val_loss:.4f}_val_acc={val_acc:.4f}', \n",
    "                                                monitor='val_acc', save_top_k=3, mode='max', save_on_train_epoch_end=False), \n",
    "                                ],\n",
    "                            logger=[\n",
    "                                    CSVLogger(        \"logs/\", name='csv/'),\n",
    "                                    TensorBoardLogger(\"logs/\", name='tensorboard/'),\n",
    "                                    ]\n",
    "\n",
    "                            \n",
    "                            )\n",
    "trainer.fit(model)\n",
    "# trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3399076706.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    L.\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "L."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs/csv/version_3'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csvlogger.log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abianche/miniconda3/envs/lightning/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at lightning_logs/convnet/version_4/checkpoints/epoch=31_step=27520_val_loss=0.0480_val_acc=val_acc=0.9922.ckpt\n",
      "Loaded model weights from the checkpoint at lightning_logs/convnet/version_4/checkpoints/epoch=31_step=27520_val_loss=0.0480_val_acc=val_acc=0.9922.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa452e61d0a74edd9b27ffc0e3d7d747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9937000274658203\n",
      "        test_loss          0.043489180505275726\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.043489180505275726, 'test_acc': 0.9937000274658203}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightning_logs/convnet/version_4/checkpoints/epoch=31_step=27520_val_loss=0.0480_val_acc=val_acc=0.9922.ckpt\n"
     ]
    }
   ],
   "source": [
    "# load best model\n",
    "# best_model = ConvNet.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "print(trainer.checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss 0.04348918\n",
      "test_acc 0.9937\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in trainer.callback_metrics.keys():\n",
    "    print(i, trainer.callback_metrics[i].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'train_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallback_metrics\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(trainer.callback_metrics['test_loss'])\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(trainer\u001b[38;5;241m.\u001b[39mcallback_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'train_loss'"
     ]
    }
   ],
   "source": [
    "print(trainer.callback_metrics['train_loss'])\n",
    "# print(trainer.callback_metrics['test_loss'])\n",
    "print(trainer.callback_metrics['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
